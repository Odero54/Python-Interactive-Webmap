{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcaf418",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Interactive webmap using python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc69af6",
   "metadata": {},
   "source": [
    "Important packages to be used are:\n",
    "    i. $GeoPandas$ to read and analyze shapefiles,\n",
    "    ii. $Folium$ to make html maps,\n",
    "    iii. $Pandas$ to handle tabular data,\n",
    "    iv. $Numpy$ to perform numerical tasks, and\n",
    "    v. $matplotlib$ to publish 2D figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e199007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading packages\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import base64\n",
    "from folium import IFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "print(\"Successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f2a75-973f-4f6a-b513-6cddd82a258c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load both attribute data and spatial data.\n",
    "\n",
    "I'm using pandas to load attribute data and geopandas to load spatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d34a3-4abe-40cb-aaff-3389b03e2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute data.\n",
    "att_data = pd.read_csv(\"cornYield.csv\")\n",
    "att_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b781178-d057-4384-8244-9de37ed6c09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial Data\n",
    "spatialData = gpd.read_file(r\"NC_USA/NorthCentral.shp\")\n",
    "spatialData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f28d6c-bb0c-413a-8926-605ad80b1dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatialData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae649f1-bc7a-4636-9516-5631741130bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatialData.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a972a8d1-1629-4e6c-86b2-3ccc6a64e636",
   "metadata": {},
   "source": [
    "### Attribute Data Cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb2780-4dce-43f5-9df8-473e23579610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the year int column into datetime\n",
    "att_data[\"Year\"] = att_data[\"Year\"].astype(\"str\") + \"-12-31\"\n",
    "att_data[\"Year\"] = pd.to_datetime(att_data.Year, format = \"%Y-%m-%d\")\n",
    "att_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d795a73-9627-4e04-b79c-8911ba6f47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "att_data = att_data.drop(columns=['County ANSI', 'Program', 'Period', 'Week Ending', 'Geo Level', 'State ANSI',\n",
    "                                  'Ag District', 'Ag District Code', 'Zip Code', 'Region', 'watershed_code', 'Watershed',\n",
    "                                 'Commodity', 'Data Item', 'Domain Category', 'Domain'])\n",
    "att_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a967f118-3344-4eec-8fbf-44d61661d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = att_data.isnull().sum()\n",
    "null_count # Total of 841 columns are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc124020-4705-4a1f-a7fc-44f8e1f495b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_data['CV (%)'] = att_data['CV (%)'].fillna(0)\n",
    "att_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b5a946-5e77-4785-9312-de6cb4cb4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = att_data.isnull().sum()\n",
    "null_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc917a92-cef8-4a09-8058-2acdac198975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a column that holds both county and state through concatenation.\n",
    "att_data['State_County'] = att_data['State'] + '_' + att_data['County']\n",
    "att_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cdecb6-affe-405d-a02f-a989f7969955",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = att_data.isnull().sum()\n",
    "null_count # our data has no null value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cfd0df-eb9d-497c-bd41-5b4ed7f14d48",
   "metadata": {},
   "source": [
    "### Calculating trend slopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce7ac07-c769-4b60-beda-a8bb9d1bbe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_slopes(data):\n",
    "    # Get unique state county combination\n",
    "    stateCountyUnique = np.unique(att_data['State_County'].values)\n",
    "    # empty dictionary to hold information\n",
    "    slopes = {}\n",
    "    #running a loop through each unique counties and calculating slope\n",
    "    for uniq in stateCountyUnique:\n",
    "        # sort by year\n",
    "        yield_val = data[att_data['State_County'] == uniq].sort_values(by='Year')\n",
    "        # Set the year as new index.\n",
    "        yield_val = yield_val.set_index('Year')\n",
    "        slope, inter, r, p, se = linregress(np.arange(1, yield_val.shape[0] + 1), yield_val.Value)\n",
    "        \n",
    "        # copy the information into the dictionary\n",
    "        slopes[uniq] = [slope, r, p]\n",
    "    \n",
    "    # create a dataframe of the dictionary\n",
    "    slopes_df = pd.DataFrame(slopes).T\n",
    "    # Change the column names\n",
    "    slopes_df.columns = ['slope', 'r', 'p']\n",
    "    # reset the index\n",
    "    slopes_df = slopes_df.reset_index()\n",
    "    \n",
    "    return slopes_df\n",
    "\n",
    "# Apply function\n",
    "slopesData = calculate_slopes(att_data)\n",
    "# save to a file\n",
    "slopesData.to_csv(r\"path_to_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4b727-83d4-4d1d-b620-61d5f97e52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's read slopesData\n",
    "# Read the slope data\n",
    "slopeData = pd.read_csv('path_to_dataframe.csv')\n",
    "slopeData.head(10)\n",
    "slopeData.rename(columns={'Unnamed: 0': 'Id', 'slope': 'Slope'}, inplace = True)\n",
    "list(slopeData)\n",
    "slopeData.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee46ce-d74e-4aa0-af6e-f69d7b473a13",
   "metadata": {},
   "source": [
    "### Creating individual trendlines of Yield at each county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15944217-bcfc-4a69-8c90-8928103a6a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating max and minimum values of the yield\n",
    "max_yield = att_data['Value'].max()\n",
    "min_yield = att_data['Value'].min()\n",
    "\n",
    "# Get unique state county combination\n",
    "stateCountyUnique = np.unique(att_data['State_County'].values)\n",
    "# stateCountyUnique\n",
    "\n",
    "# run for loop to get figure in each county\n",
    "for uniq in stateCountyUnique:\n",
    "    yield_val = att_data[att_data['State_County'] == uniq].sort_values(by='Year')\n",
    "    yield_val = yield_val.set_index('Year')\n",
    "    slope, inter, r, p, se = linregress(np.arange(1, yield_val.shape[0] + 1), yield_val.Value)\n",
    "    \n",
    "    # create a figure plot\n",
    "    fig, ax = plt.subplots(1,1, figsize=(2,2), dpi=72)\n",
    "    ax.plot(yield_val['Value'], color='blue')\n",
    "    ax.set_ylabel('Yield(Bu/Acre)')\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylim(bottom=min_yield, top=max_yield)\n",
    "    ax.text(0.04, 0.96,\n",
    "            f\"{yield_val['County'][0].capitalize()} County, {yield_val['State'][0].capitalize()}\",\n",
    "            ha='left', va='top', transform=ax.transAxes, fontsize=11)\n",
    "    ax.text(0.04, 0.08, f\"Trend Slope: {slope:.2f}\\np-value: {p:.3f}\",\n",
    "           ha='left', va='top', transform = ax.transAxes)\n",
    "    # save the figure\n",
    "    outName = rf\"Plots\\{slopeData['index'][0]+'.png'}\"\n",
    "    fig.savefig(outName, dpi=72, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0af76-e48d-44cb-91c1-73ee7d5d4131",
   "metadata": {},
   "source": [
    "### The Fun part, Web Mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3227339-c54a-4ac7-9305-0bd8398ac859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the slope data\n",
    "data = slopeData\n",
    "\n",
    "\n",
    "# read shapefile data\n",
    "gpd_data = gpd.read_file(r\"NC_USA/NorthCentral.shp\")\n",
    "gpd_data.to_file('myshpfile.geojson', driver='GeoJSON')\n",
    "\n",
    "shape_geo = gpd.read_file(r\"myshpfile.geojson\")\n",
    "# shape_geo.head()\n",
    "\n",
    "shape_geo=shape_geo[['FID_Georgi', 'NAME_1', 'NAME_2', 'geometry']]\n",
    "shape_geo.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e89ae2-bf9d-4547-a3b6-22f79ecdeb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_geo.rename(columns = {'FID_Georgi':'ID', 'NAME_1':'State', 'NAME_2': 'County'}, inplace = True)\n",
    "shape_geo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abbf18-ca35-4a97-8d68-efc1d0c497f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'Id':'ID'}, inplace = True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981fb29-6e91-4774-8eb7-71493fa8169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adding new column\n",
    "# id = [i for i in range(len(shape_geo.axes[0]))]\n",
    "# shape_geo['Id'] = id\n",
    "\n",
    "# overlaying our spatial data to slopedata which store information about\n",
    "# our location data\n",
    "mergedData = shape_geo.merge(data, on='ID')\n",
    "\n",
    "# Adding more columns\n",
    "# County = [county for county in att_data['County'].values]\n",
    "# States = [states for states in att_data['State'].values]\n",
    "# mergedData['County'] = County\n",
    "# mergedData['States'] = States\n",
    "mergedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832eb9cd-6561-45e5-bdf1-4130a742a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "png = []\n",
    "directory = r\"Plots\"\n",
    "for png_s in os.scandir(directory):\n",
    "    if png_s.is_file():\n",
    "        png.append(png_s.name)\n",
    "\n",
    "    print(png)\n",
    "\n",
    "mergedData['images'] = png\n",
    "mergedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e4d10-c5d8-4a86-84dd-6e40cda78fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_map=shape_geo.centroid.x.mean()\n",
    "y_map=shape_geo.centroid.y.mean()\n",
    "\n",
    "# Create a custom scale for the legend\n",
    "# I'm using the quantile classification to make a custom scale\n",
    "\n",
    "customScale = (data['Slope'].quantile((0, 0.2, 0.4, 0.6, 0.8, 1))).tolist()\n",
    "\n",
    "# Initialize the map and store it in a foliumMap object\n",
    "foliumMap = folium.Map(location=[y_map, x_map], zoom_start=6)\n",
    "\n",
    "# This is the folium object\n",
    "ch = folium.Choropleth(\n",
    "     geo_data = shape_geo, \n",
    "     data = mergedData, \n",
    "     columns = ['ID', 'Slope'], # specify columns you need to use, we only need these two\n",
    "     key_on = 'feature.properties.ID', # specify Id column, which is unique\n",
    "     bins = customScale, # defining the scale\n",
    "     fill_color='YlOrRd', # Defining the color scale\n",
    "     nan_fill_color = 'white',\n",
    "     fill_opacity = 0.7, # some transparency on the fill color\n",
    "     line_opacity = 0.2, # some fill color on the line color\n",
    "     legend_name = 'Trend', # legend name\n",
    "     highlight = True, # highlighting shape when hovering\n",
    "     line_color = 'black').add_to(foliumMap) # Add this object to the defined map\n",
    "\n",
    "foliumMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6dd04-15b3-4a3f-98de-5026966d0a24",
   "metadata": {},
   "source": [
    "### Let's add some popup to our map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8611a-1c35-4633-982d-7111d4635c13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# assign directory\n",
    "directory = r\"Plots\"\n",
    "# iterate over files in\n",
    "# that directory\n",
    "# for imageFiles in os.scandir(directory):\n",
    "#     print(imageFiles.name)\n",
    "\n",
    "# run a for loop for every raw in the dataframe\n",
    "for i in range(mergedData.shape[0]):\n",
    "    image_name = mergedData.loc[i, 'images']\n",
    "    image_path = os.path.join(r\"Plots\", image_name)\n",
    "    encoded = base64.b64encode(open(image_path, 'rb').read())\n",
    "    html = '<img src=\"data:image/png;base64,{}\">'.format\n",
    "    width, height = 4, 4 # smaller width and height of each figure\n",
    "    resolution = 72 # Little lower resolution\n",
    "    \n",
    "    # Create the frame, note that we are adding some values(20 and 30).\n",
    "    # This will help user to avoid scrolling in the popped up figures\n",
    "    iframe = IFrame(html(encoded.decode(\"UTF-8\")), width=(width * resolution)+20, height=(height * resolution)+30)\n",
    "    popup = folium.Popup(iframe, maxWidth=2650) # This is the popup object.\n",
    "    \n",
    "    # The style function is important. This is a lambda function with style properties.\n",
    "    # It says the clickable polygon should not have any fill or line colors\n",
    "    style_function = lambda x: {'fillColor': '#ffffff',\n",
    "                                'color': '#000000',\n",
    "                                'fillOpacity': 0.1,\n",
    "                                'weight': 0.1}\n",
    "    \n",
    "    # Create a geojson object of each polygon. Note that, this is practically invisible.\n",
    "    b = folium.GeoJson(shape_geo.iloc[i, 3], style_function=style_function)\n",
    "    b.add_child(popup) # Add the popup to it\n",
    "    ch.add_child(b) # Add the geojson into the map\n",
    "foliumMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a11f2-1b04-4a4f-a47c-e1d677ab0d77",
   "metadata": {},
   "source": [
    "### Adding a title and saving as htmlfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9444e7fc-0518-4a72-acfe-c2c8f8b89cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adding map title\n",
    "text = 'A 10-year Spatiotemporal Trend of Yield in Georgia State'\n",
    "title_html = '''\n",
    "                <h3>align=\"left\" style=\"font-size:22px\"><b>{}</b></h3>\n",
    "             '''.format(text)\n",
    "\n",
    "foliumMap.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "# Save the html file\n",
    "foliumMap.save(r\"Result/cornYieldMap.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
